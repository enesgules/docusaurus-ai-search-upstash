---
title: "LlamaIndex with Upstash Vector"
---
You can use LlamaIndex with Upstash Vector to perform Retrieval-Augmented Generation (RAG). LlamaIndex is a powerful tool that integrates seamlessly with vector databases like Upstash Vector, enabling advanced query and response capabilities.
## Install
```bash
pip install llama-index upstash-vector llama-index-vector-stores-upstash python-dotenv
```
## Setup
First, create a Vector Index in the [Upstash Console](https://console.upstash.com). Configure the index with:
- **Dimensions**: 1536
- **Distance Metric**: Cosine
Once the index is created, copy the `UPSTASH_VECTOR_REST_URL` and `UPSTASH_VECTOR_REST_TOKEN` and add them to your `.env` file along with your OpenAI API key:
```
UPSTASH_VECTOR_REST_URL=your_upstash_url
UPSTASH_VECTOR_REST_TOKEN=your_upstash_token
OPENAI_API_KEY=your_openai_api_key
```
## Usage
Hereâ€™s how you can integrate LlamaIndex with Upstash Vector:
```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.vector_stores.upstash import UpstashVectorStore
from llama_index.core import StorageContext
import os
from dotenv import load_dotenv
# Load environment variables
load_dotenv()
# Set OpenAI API key
openai.api_key = os.environ["OPENAI_API_KEY"]
# Initialize Upstash Vector store
upstash_vector_store = UpstashVectorStore(
    url=os.environ["UPSTASH_VECTOR_REST_URL"],
    token=os.environ["UPSTASH_VECTOR_REST_TOKEN"],
)
# Load documents using SimpleDirectoryReader
documents = SimpleDirectoryReader("./documents/").load_data()
# Create a storage context and initialize the index
storage_context = StorageContext.from_defaults(vector_store=upstash_vector_store)
index = VectorStoreIndex.from_documents(
    documents, storage_context=storage_context
)
```
## Querying
Once the index is created, you can query it to retrieve and generate responses based on document content.
```python
# Initialize the query engine
query_engine = index.as_query_engine()
# Perform queries
response_1 = query_engine.query("What is global warming?")
print(response_1)
response_2 = query_engine.query("How can we reduce our carbon footprint?")
print(response_2)
```
## Notes
- You can specify a namespace when creating the `UpstashVectorStore` instance:
  ```python
  vector_store = UpstashVectorStore(
      url="your_upstash_url",
      token="your_upstash_token",
      namespace="your_namespace"
  )
  ```
- Visit the [LlamaIndex documentation](https://docs.llamaindex.ai/en/latest) for more details.